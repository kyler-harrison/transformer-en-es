{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92dd09df",
   "metadata": {},
   "source": [
    "### This notebook reduces the english-spanish dataset so that any sentences that contain tokens that are not contained in the english or spanish word2vec dictionaries are removed. It also reduces the size of the english and spanish word2vec dictionaries to only contain needed tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c1ea154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from preprocess import sentence_to_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1925e34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Ve.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vete.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vaya.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Váyase.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hola.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  english  spanish\n",
       "0     Go.      Ve.\n",
       "1     Go.    Vete.\n",
       "2     Go.    Vaya.\n",
       "3     Go.  Váyase.\n",
       "4     Hi.    Hola."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load en-es string pair data\n",
    "data = pd.read_csv(\"en_es_data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec86a9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize strings into lists of tokens\n",
    "# see preprocess.py for function definition\n",
    "data[\"en_tokens\"] = data[\"english\"].apply(lambda sent: sentence_to_tokens(sent, \"en\"))\n",
    "data[\"es_tokens\"] = data[\"spanish\"].apply(lambda sent: sentence_to_tokens(sent, \"es\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88ea52d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>spanish</th>\n",
       "      <th>en_tokens</th>\n",
       "      <th>es_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Ve.</td>\n",
       "      <td>[&lt;s&gt;, go, ., &lt;e&gt;]</td>\n",
       "      <td>[&lt;s&gt;, ve, ., &lt;e&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vete.</td>\n",
       "      <td>[&lt;s&gt;, go, ., &lt;e&gt;]</td>\n",
       "      <td>[&lt;s&gt;, vete, ., &lt;e&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vaya.</td>\n",
       "      <td>[&lt;s&gt;, go, ., &lt;e&gt;]</td>\n",
       "      <td>[&lt;s&gt;, vaya, ., &lt;e&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Váyase.</td>\n",
       "      <td>[&lt;s&gt;, go, ., &lt;e&gt;]</td>\n",
       "      <td>[&lt;s&gt;, váyase, ., &lt;e&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hola.</td>\n",
       "      <td>[&lt;s&gt;, hi, ., &lt;e&gt;]</td>\n",
       "      <td>[&lt;s&gt;, hola, ., &lt;e&gt;]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  english  spanish          en_tokens              es_tokens\n",
       "0     Go.      Ve.  [<s>, go, ., <e>]      [<s>, ve, ., <e>]\n",
       "1     Go.    Vete.  [<s>, go, ., <e>]    [<s>, vete, ., <e>]\n",
       "2     Go.    Vaya.  [<s>, go, ., <e>]    [<s>, vaya, ., <e>]\n",
       "3     Go.  Váyase.  [<s>, go, ., <e>]  [<s>, váyase, ., <e>]\n",
       "4     Hi.    Hola.  [<s>, hi, ., <e>]    [<s>, hola, ., <e>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "242b499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load en and es word2vec dictionaries\n",
    "en_vec_df = pd.read_csv(\"cc.en.300.csv\")\n",
    "es_vec_df = pd.read_csv(\"cc.es.300.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9fa2d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [word, vector]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [word, vector]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [word, vector]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [word, vector]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# these should all be empty\n",
    "print(en_vec_df[en_vec_df[\"word\"] == \"<s>\"])\n",
    "print(en_vec_df[en_vec_df[\"word\"] == \"<e>\"])\n",
    "print(es_vec_df[es_vec_df[\"word\"] == \"<s>\"])\n",
    "print(es_vec_df[es_vec_df[\"word\"] == \"<e>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58b56ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add special start <s> and end <e> tokens\n",
    "vec_size = 300\n",
    "start_token = \"<s>\"\n",
    "end_token = \"<e>\"\n",
    "\n",
    "# these were decided arbitrarily\n",
    "start_vec = np.array([1.0 for i in range(vec_size)])\n",
    "end_vec = np.array([0.1 for i in range(vec_size)])\n",
    "\n",
    "# add to english\n",
    "en_vec_df = pd.concat([en_vec_df, pd.DataFrame({\"word\": [start_token, end_token], \"vector\": [start_vec, end_vec]})])\n",
    "en_vec_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# add to spanish\n",
    "es_vec_df = pd.concat([es_vec_df, pd.DataFrame({\"word\": [start_token, end_token], \"vector\": [start_vec, end_vec]})])\n",
    "es_vec_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd0d54ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1999997</th>\n",
       "      <td>hvm</td>\n",
       "      <td>[-0.0634 -0.0375 -0.2048 -0.0199 0.2529 0.2086...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999998</th>\n",
       "      <td>GorceyBearTerritory.netSaturday</td>\n",
       "      <td>[0.0142 0.0230 -0.0099 -0.0223 -0.0068 -0.0091...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999999</th>\n",
       "      <td>Zwicke</td>\n",
       "      <td>[-0.0499 0.0152 0.0038 -0.0695 -0.0220 -0.0079...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000000</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000001</th>\n",
       "      <td>&lt;e&gt;</td>\n",
       "      <td>[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    word  \\\n",
       "1999997                              hvm   \n",
       "1999998  GorceyBearTerritory.netSaturday   \n",
       "1999999                           Zwicke   \n",
       "2000000                              <s>   \n",
       "2000001                              <e>   \n",
       "\n",
       "                                                    vector  \n",
       "1999997  [-0.0634 -0.0375 -0.2048 -0.0199 0.2529 0.2086...  \n",
       "1999998  [0.0142 0.0230 -0.0099 -0.0223 -0.0068 -0.0091...  \n",
       "1999999  [-0.0499 0.0152 0.0038 -0.0695 -0.0220 -0.0079...  \n",
       "2000000  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  \n",
       "2000001  [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, ...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vec_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "757e4885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1999997</th>\n",
       "      <td>blaciones</td>\n",
       "      <td>[0.0032 0.0005 0.0574 -0.0039 0.0159 0.0112 -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999998</th>\n",
       "      <td>LDSInfantiles</td>\n",
       "      <td>[-0.0071 0.0046 0.0645 0.0001 -0.0075 -0.0288 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999999</th>\n",
       "      <td>TEDxQuito</td>\n",
       "      <td>[-0.0197 -0.0194 0.0922 0.0005 -0.0080 -0.0649...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000000</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000001</th>\n",
       "      <td>&lt;e&gt;</td>\n",
       "      <td>[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  word                                             vector\n",
       "1999997      blaciones  [0.0032 0.0005 0.0574 -0.0039 0.0159 0.0112 -0...\n",
       "1999998  LDSInfantiles  [-0.0071 0.0046 0.0645 0.0001 -0.0075 -0.0288 ...\n",
       "1999999      TEDxQuito  [-0.0197 -0.0194 0.0922 0.0005 -0.0080 -0.0649...\n",
       "2000000            <s>  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...\n",
       "2000001            <e>  [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, ..."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_vec_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fce13ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_tokens(data, tokenized_col):\n",
    "    \"\"\"\n",
    "    Gets the unique set of tokens contained in all tokenized lists in\n",
    "    data[tokenized_col]\n",
    "    \n",
    "    data (pd.DataFrame): en-es dataset\n",
    "    tokenized_col: name of column containing tokenized lists\n",
    "    \"\"\"\n",
    "    \n",
    "    # this might not be the most efficient method, but it works fine\n",
    "    all_tokens_ls = data[tokenized_col].values.tolist()\n",
    "    all_tokens = []\n",
    "\n",
    "    for token_ls in all_tokens_ls:\n",
    "        for token in token_ls:\n",
    "            all_tokens.append(token)\n",
    "\n",
    "    unique_tokens = list(set(all_tokens))\n",
    "    \n",
    "    return unique_tokens\n",
    "\n",
    "\n",
    "def check_token(invalid_tokens, token_ls):\n",
    "    \"\"\"\n",
    "    Checks if any tokens in token_ls is in invalid_tokens.\n",
    "    Returns True at first match, else if no match found.\n",
    "    \"\"\"\n",
    "    \n",
    "    for token in token_ls:\n",
    "        if token in invalid_tokens:\n",
    "            return True    \n",
    "    return False\n",
    "\n",
    "\n",
    "def reduce_data(data, vec_df, unique_tokens, tokenized_col):\n",
    "    \"\"\"\n",
    "    Reduces data to only contain rows where all tokens are also found\n",
    "    in vec_df. Creates and returns a new reduced dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    # create dataframe out of list of unique tokens\n",
    "    set_df = pd.DataFrame({\"word\": unique_tokens})\n",
    "    \n",
    "    # get column of True/False if token in word2vec dict\n",
    "    set_df[\"in_vec\"] = set_df[\"word\"].isin(vec_df[\"word\"])\n",
    "    \n",
    "    # get dataframe containing only words NOT contained in word2vec dict\n",
    "    invalid_indexes = set_df[set_df[\"in_vec\"] == False].index\n",
    "    invalid_df = set_df.iloc[invalid_indexes, :]\n",
    "    \n",
    "    # convert to list for search purposes (maybe not that clever)\n",
    "    invalid_tokens = invalid_df[\"word\"].values.tolist()\n",
    "    \n",
    "    # put boolean in og dataframe indicating whether tokenized list \n",
    "    # contains a word not contained \n",
    "    data[f\"{tokenized_col}_has_invalid_token\"] = data[tokenized_col].apply(lambda token_ls: check_token(invalid_tokens, token_ls))\n",
    "\n",
    "    # create new dataframe w/o any rows containing invalid tokens in tokenized_col\n",
    "    reduced_data = data[data[f\"{tokenized_col}_has_invalid_token\"] == False]\n",
    "\n",
    "    return reduced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3430f004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of unique tokens for each language across entire dataset\n",
    "en_unique_tokens = get_unique_tokens(data, \"en_tokens\")\n",
    "es_unique_tokens = get_unique_tokens(data, \"es_tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f59e03d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13598 unique english tokens\n",
      "26110 unique spanish tokens\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(en_unique_tokens)} unique english tokens\")\n",
    "print(f\"{len(es_unique_tokens)} unique spanish tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a65f7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape before any reduction: (118964, 4)\n",
      "data shape after english reduction: (111713, 5)\n",
      "data shape after spanish reduction: (111184, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3388/2998804586.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[f\"{tokenized_col}_has_invalid_token\"] = data[tokenized_col].apply(lambda token_ls: check_token(invalid_tokens, token_ls))\n"
     ]
    }
   ],
   "source": [
    "# remove all rows in data where there are tokens in either en or es\n",
    "# tokenization that are not contained in corresponding word2vec dict\n",
    "# reduce based on english strings first\n",
    "print(f\"data shape before any reduction: {data.shape}\")\n",
    "reduced_data = reduce_data(data, en_vec_df, en_unique_tokens, \"en_tokens\")\n",
    "print(f\"data shape after english reduction: {reduced_data.shape}\")\n",
    "\n",
    "# now reduce based on spanish strings\n",
    "reduced_data = reduce_data(reduced_data, es_vec_df, es_unique_tokens, \"es_tokens\")\n",
    "print(f\"data shape after spanish reduction: {reduced_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3cb67846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_vec(unique_tokens, vec_df):\n",
    "    \"\"\"\n",
    "    Returns a df that is a reduced version of vec_df. Only tokens\n",
    "    contained in unique_tokens will remain in this reduced version.\n",
    "    \"\"\"\n",
    "    \n",
    "    # set bool (not to be confused with set) if word in vec (should already be true always)\n",
    "    set_df = pd.DataFrame({\"word\": unique_tokens})\n",
    "    set_bool = set_df[\"word\"].isin(vec_df[\"word\"])\n",
    "    \n",
    "    # sanity check (sizes should be the same)\n",
    "    print(f\"unique tokens count: {len(unique_tokens)}\")\n",
    "    print(f\"true count: {set_bool[set_bool == True].size}\")\n",
    "    \n",
    "    # set bool if word in vec is in the set of unique tokens\n",
    "    vec_bool = vec_df[\"word\"].isin(set_df[\"word\"])\n",
    "    \n",
    "    # only keep rows in vec where word is used\n",
    "    red_vec_df = vec_df.iloc[vec_bool[vec_bool == True].index, :]\n",
    "    \n",
    "    # sanity check again (should be same as above)\n",
    "    print(f\"num rows in reduced vec_df: {red_vec_df.shape[0]}\")\n",
    "    \n",
    "    return red_vec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "71eb87c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique tokens count: 12664\n",
      "true count: 12664\n",
      "num rows in reduced vec_df: 12664\n",
      "unique tokens count: 24672\n",
      "true count: 24672\n",
      "num rows in reduced vec_df: 24672\n"
     ]
    }
   ],
   "source": [
    "# get new lists of unique tokens \n",
    "en_red_unique = get_unique_tokens(reduced_data, \"en_tokens\")\n",
    "es_red_unique = get_unique_tokens(reduced_data, \"es_tokens\")\n",
    "\n",
    "# reduce vector dictionaries\n",
    "en_red_vec = reduce_vec(en_red_unique, en_vec_df)\n",
    "es_red_vec = reduce_vec(es_red_unique, es_vec_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "75546e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write reduced dataset (NOTE: keeping tokenization as a separate process, \n",
    "# entire purpose of this nb was just to reduce cost of dataset and\n",
    "# word2vec dictionaries, even though the code sort of \"skips ahead\"\n",
    "# into the actual data transformation steps)\n",
    "reduced_data[[\"english\", \"spanish\"]].to_csv(\"en_es_reduced_data.csv\", index=False)\n",
    "\n",
    "# write reduced vector dictionaries\n",
    "en_red_vec.to_csv(\"cc.en.300.reduced.csv\", index=False)\n",
    "es_red_vec.to_csv(\"cc.es.300.reduced.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59330e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfrmr-kernel",
   "language": "python",
   "name": "tfrmr-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
